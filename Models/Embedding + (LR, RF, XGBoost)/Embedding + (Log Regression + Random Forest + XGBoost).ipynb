{"cells":[{"cell_type":"markdown","metadata":{"id":"7mYsT7fJmj4b"},"source":["#Init."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2676,"status":"ok","timestamp":1659013652921,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"},"user_tz":-120},"id":"GuA7OFVyGcuL","outputId":"cd85988a-97d8-4aba-9006-6fe879fa4826"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/.shortcut-targets-by-id/1NSBJKoA0FA5BeDWR8bNo5tPgA5lCU8yv/CIL 2022\n","'basic urban dictionary+21. July RandomForestClassifier(n_estimators=128, random_state=0, n_jobs=-1)with-stemming_with-lemmatize_no-stopwords_with-spellcorrect.txt'\n","'basic urban dictionary+21. July RandomForestClassifier(n_estimators=128, random_state=0, n_jobs=-1)with-stemming_with-lemmatize_with-stopwords_no-spellcorrect.txt'\n","'Bert inspiration'\n"," BoW_v2.ipynb\n"," code\n"," data\n"," fasttext+LSTM_fasttext_rawraw.txt\n"," finetuned_roberta_model\n"," finetuned_roberta_model_in_steps\n"," finetuned_roberta_model_in_steps_val.csv\n"," finetuned_roberta_model_in_steps_with_dropout\n","'For Euler Cluster'\n","'GRU4_stanford glove_raw_test.csv'\n","'GRU4_stanford glove_raw_train.csv'\n","'GRU4_stanford glove_raw_val.csv'\n","'Grubert stats and model'\n","'Grubert v.A.1.;epochnr=2 time_duraction=12342.342999219894s'\n","'Grubert v.A.1.;epochnr=3 time_duraction=12331.698219776154s'\n","'Grubert v.A.1.;epochnr=4 time_duraction=12342.842751264572s'\n","'Grubert v.A.1.;epochnr=5 time_duraction=12338.235754489899s'\n","'Grubert v.A.1. - Gabriel - full dataset;epochnr=0 time_duraction=22675.283185005188s'\n","'Grubert v.A.1. - Gabriel - full dataset;epochnr=1 time_duraction=22671.999267339706s'\n","'Grubert v.A.1. - Gabriel - full dataset;epochnr=3 time_duraction=16164.560936450958s'\n","'Grubert v.A.1. - Gabriel - full dataset;stats.txt'\n","'Grubert v.A.1. - Oliver - limit 250000;stats.txt'\n","'Grubert v.A.1.;stats.txt'\n","'Grubert v.A.1._val.csv'\n","'Joaquin Models'\n","'Link to latex file.gdoc'\n"," Models\n"," Notes.gdoc\n"," On_Processed_Bag-of-words_baseline.ipynb\n"," papers_to_possible_reference\n","'phrase urban dictionary+1-LogisticRegression-C=1-max_iter=1000no-stemming_no-lemmatize_no-stopwords_no-spellcorrect.txt'\n","'phrase urban dictionary+2-LogisticRegression-C=1-max_iter=1000no-stemming_no-lemmatize_with-stopwords_no-spellcorrect.txt'\n","'phrase urban dictionary+3-LogisticRegression-C=1-max_iter=1000no-stemming_no-lemmatize_with-stopwords_with-spellcorrect.txt'\n","'phrase urban dictionary+4-LogisticRegression-C=1-max_iter=1000no-stemming_with-lemmatize_with-stopwords_no-spellcorrect.txt'\n","'phrase urban dictionary+5-LogisticRegression-C=1-max_iter=1000no-stemming_with-lemmatize_with-stopwords_with-spellcorrect.txt'\n","'phrase urban dictionary+6-LogisticRegression-C=1-max_iter=1000with-stemming_no-lemmatize_with-stopwords_no-spellcorrect.txt'\n","'phrase urban dictionary+7-LogisticRegression-C=1-max_iter=1000with-stemming_with-lemmatize_no-stopwords_with-spellcorrect.txt'\n","'phrase urban dictionary+8-LogisticRegression-C=1-max_iter=1000with-stemming_with-lemmatize_with-stopwords_no-spellcorrect.txt'\n"," Preprocessing\n"," Project_2_Bag-of-words_baseline.ipynb\n"," __pycache__\n"," resources\n"," Results.gsheet\n","'stanford glove+21. July RandomForestClassifier(n_estimators=128, random_state=0, n_jobs=-1)with-stemming_with-lemmatize_no-stopwords_with-spellcorrect.txt'\n","'stanford glove+21. July RandomForestClassifier(n_estimators=128, random_state=0, n_jobs=-1)with-stemming_with-lemmatize_with-stopwords_no-spellcorrect.txt'\n","'stanford glove+GRU4_stanford glove_rawraw.txt'\n","'stanford glove+GRU4v2_stanford glove_rawraw.txt'\n","'stanford glove+GRU_stanford glove_rawraw.txt'\n","'stanford glove+XGB Classifier-embedding=stanford glove;-depth=12estimators=128with-stemming_with-lemmatize_with-stopwords_no-spellcorrect.txt'\n"," template.ipynb\n","'trained Glove_LogisticRegression(C=1, max_iter=1000).pkl'\n","'trained GRU4_stanford glove_raw.pkl'\n","'trained GRU4v2_stanford glove_raw.pkl'\n","'trained GRU_stanford glove_raw.pkl'\n","'trained LSTM_fasttext_raw.pkl'\n"," trash\n","'Word embeddings'\n","'XGBoost stats and models'\n","'XLNet stats and model'\n","'XLNET v.A.2 - hyperparametertuning_more_trainingdata;BCEWithLogitsLoss;stats.txt'\n","'XLNET v.A.2 - hyperparametertuning_more_trainingdata;Piotrek;epochnr=0 time_duraction=10746.346230268478s'\n","'XLNET v.A.2 - hyperparametertuning_more_trainingdata;Piotrek;epochnr=1 time_duraction=10712.524363040924s'\n","'XLNET v.A.2 - hyperparametertuning_more_trainingdata;Piotrek;stats.txt'\n","'XLNET v.A.2 - raw;fixed_token-hyperparametertuning_more_trainingdata;Piotrek;epochnr=0 time_duraction=11560.570780992508s'\n","'XLNET v.A.2 - raw;fixed_token-hyperparametertuning_more_trainingdata;Piotrek;stats.txt'\n","'XLNET v.A.2 - raw;hyperparametertuning_more_trainingdata;Piotrek;epochnr=0 time_duraction=11703.232524394989s'\n","'XLNET v.A.2 - raw;hyperparametertuning_more_trainingdata;Piotrek;epochnr=1 time_duraction=11687.268048286438s'\n","'XLNET v.A.2 - raw;hyperparametertuning_more_trainingdata;Piotrek;stats.txt'\n"]}],"source":["#In this cell, there are various option for the user to freely choose:\n","#embedding_choice\n","#PREPROCESSING_CHOICE\n","\n","import pandas as pd\n","import numpy as np\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.tokenize import TweetTokenizer\n","tknzr = TweetTokenizer()\n","\n","from gensim.models import Word2Vec\n","from gensim.models import FastText\n","from gensim.models import KeyedVectors\n","#from gensim.test.utils import common_texts\n","import gensim.downloader as api\n","import gensim\n","\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n","\n","use_drive = True\n","if use_drive:\n","  PATH = \"/content/drive/MyDrive/CIL 2022/\"\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  %cd /content/drive/MyDrive/CIL 2022/\n","  !ls\n","else:\n","  PATH = \"./\"\n","\n","#Choose embedding below\n","#1 = stanford glove\n","#2 = word2vec\n","#3 = fasttext\n","#4 = basic urban dictionary embedding\n","#5 = phrase urban dictionary embedding\n","embedding_choice = 1\n","\n","USE_DICTIONARY = True\n","embedding_non_dict_model = None\n","embedding_dict = dict()\n","#SET FLAG \"USE_DICTIONARY\" to determine if we work with a dictionary or a loaded pretrained model\n","if (embedding_choice == 2) or (embedding_choice == 3):\n","  USE_DICTIONARY = False\n","\n","# HYPERPARAMETERS\n","# LOAD_FROM_SPLITTED_DATASET = True\n","# WRITE_SPLITTED_DATASET = True\n","# REMOVE_DUPLICATES_FROM_TRAINING = False\n","\n","#option 0 - 8\n","PREPROCESSING_OPTIONS = [ \"raw\",\n","\"no-stemming_no-lemmatize_no-stopwords_no-spellcorrect\",\n","\"no-stemming_no-lemmatize_with-stopwords_no-spellcorrect\",\n","\"no-stemming_no-lemmatize_with-stopwords_with-spellcorrect\",\n","\"no-stemming_with-lemmatize_with-stopwords_no-spellcorrect\",\n","\"no-stemming_with-lemmatize_with-stopwords_with-spellcorrect\",\n","\"with-stemming_no-lemmatize_with-stopwords_no-spellcorrect\",\n","\"with-stemming_with-lemmatize_no-stopwords_with-spellcorrect\",\n","\"with-stemming_with-lemmatize_with-stopwords_no-spellcorrect\" ]\n","PREPROCESSING_CHOICE = PREPROCESSING_OPTIONS[0] # one from PREPROCESSING_OPTIONS\n","\n","\n","#helper function to save models and stats\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n","import pickle\n","\n","#save model\n","# now you can save it to a file\n","# with open('filename.pkl', 'wb') as f:\n","#     pickle.dump(clf, f)\n","\n","# # and later you can load it\n","# with open('filename.pkl', 'rb') as f:\n","#     clf = pickle.load(f)\n","\n","#1 = stanford glove\n","#2 = word2vec\n","#3 = fasttext\n","#4 = basic urban dictionary embedding\n","#5 = phrase urban dictionary embedding\n","def get_embedding_name(embedding_id):\n","  if embedding_id == 1:\n","    return \"stanford glove\"\n","  elif embedding_id == 2:\n","    return \"word2vec\"\n","  elif embedding_id == 3:\n","    return \"fasttext\"\n","  elif embedding_id == 4:\n","    return \"basic urban dictionary\"\n","  elif embedding_id == 5:\n","    return \"phrase urban dictionary\"\n","  else:\n","    return \"User error: embedding id does not exist.\"\n","\n","#Saving trained model\n","def save_model(model, model_name):\n","  name_of_file_model = \"trained \" + model_name + \".pkl\"\n","  with open(name_of_file_model, 'wb') as f:\n","    pickle.dump(model, f)\n","  print(\"Model saved.\")\n","\n","#Saving stats of model to a file\n","def save_stats_to_file(y_val, y_val_pred, model_name):\n","  name_of_file_stats = get_embedding_name(embedding_choice) + \"+\" + model_name + PREPROCESSING_CHOICE + \".txt\"\n","  print(\"Saving model relevant stats to file.\")\n","  file_obj = open(name_of_file_stats, \"w\", encoding=\"utf8\")\n","  file_obj.write(f'use_drive: {use_drive}\\n')\n","  file_obj.write(f'PREPROCESSING_CHOICE: {PREPROCESSING_CHOICE}\\n')\n","  file_obj.write(f'embedding_choice: {embedding_choice}\\n')\n","  file_obj.write(f'embedding_name: {get_embedding_name(embedding_choice)}\\n')\n","  file_obj.write(f'dimension_of_embedding: {dimension_of_embedding}\\n')\n","  file_obj.write(f'Model name: {model_name}\\n')\n","  file_obj.write(f'Acc: {accuracy_score(y_val, y_val_pred)}\\n')\n","  file_obj.write(f'Recall: {recall_score(y_val, y_val_pred)}\\n')\n","  file_obj.write(f'Precision: {precision_score(y_val, y_val_pred)}\\n')\n","  file_obj.write(f'F1: {f1_score(y_val, y_val_pred)}\\n')\n","  file_obj.write(f'ROC_AUC: {roc_auc_score(y_val, y_val_pred)}\\n')\n","  file_obj.close()\n","  print(f'Acc: {accuracy_score(y_val, y_val_pred)}')\n","  print(f'Recall: {recall_score(y_val, y_val_pred)}')\n","  print(f'Precision: {precision_score(y_val, y_val_pred)}')\n","  print(f'F1: {f1_score(y_val, y_val_pred)}')\n","  print(f'ROC_AUC: {roc_auc_score(y_val, y_val_pred)}')"]},{"cell_type":"markdown","metadata":{"id":"gK44dOWs-o-T"},"source":["#Choose embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6BN0Yc_pJR4"},"outputs":[],"source":["#From the glove embedding file\n","\n","#https://github.com/RaRe-Technologies/gensim-data#datasets\n","#https://kavita-ganesan.com/easily-access-pre-trained-word-embeddings-with-gensim/#.YsGEXexBxE4\n","\n","#depending on choice, we choose a different file/embedding\n","if embedding_choice == 1:\n","  #stanford\n","  #files offer dimension_of_embedding = {25,50,100,200}\n","  dimension_of_embedding = 200 #can also modify this\n","\n","  #alternativly via gensim:\n","  # model_name = \"glove-twitter-\" + str(dimension_of_embedding)\n","  # model = api.load(model_name)\n","\n","  #load pre-trained embedding directly from stanford glove file:\n","  path_to_embedding_file = PATH + \"Word embeddings/\" #navigate to folder\n","  path_to_embedding_file += \"Glove/glove.twitter.27B.\" + str(dimension_of_embedding) + \"d.txt\"\n","\n","  embedding_file_obj = open(path_to_embedding_file, \"r\", encoding = \"utf8\")\n","  for line in embedding_file_obj:\n","    #Check if line consists only of ASCII characters (no Chinese/Japanse/Arabic, etc. expressions)\n","    #There were some lines that had non-ASCII characters.\n","    if line.isascii():\n","      word_n_vector = line.split()\n","      embedding_dict[word_n_vector[0]] = np.array(word_n_vector[1:], dtype=np.float64)\n","\n","  embedding_file_obj.close\n","\n","  #add <UNKNOWN> token to vocabulary\n","  #choose the 0 vector as the corresponding embedding\n","  embedding_dict[\"<UNKNOWN>\"] = np.zeros(dimension_of_embedding, dtype=np.float64)\n","\n","elif embedding_choice == 2:\n","  \n","#https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n","\n","  #via gensim:\n","  dimension_of_embedding = 300\n","  \n","  #Load pre-trained embedding via gensim\n","  model_name = \"word2vec-google-news-\" + str(dimension_of_embedding)\n","  embedding_non_dict_model = api.load(model_name) #load pretrained model\n","\n","elif embedding_choice == 3:\n","\n","#https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py\n","  \n","  dimension_of_embedding = 300\n","\n","  #Load pre-trained embedding via gensim\n","  model_name = \"fasttext-wiki-news-subwords-\" + str(dimension_of_embedding)\n","  embedding_non_dict_model = api.load(model_name) #load pretrained model\n"," \n","elif (embedding_choice == 4) or (embedding_choice == 5):\n","  \n","  #see https://aclanthology.org/2020.lrec-1.586.pdf for difference.\n","  path_to_embedding_file = PATH + \"Word embeddings/\"\n","  if embedding_choice == 4:\n","    path_to_embedding_file += \"urbanDictionary/ud_basic.vec\"\n","  else:\n","    path_to_embedding_file += \"urbanDictionary/ud_phrase.vec\"\n","  \n","  embedding_file_obj = open(path_to_embedding_file, \"r\")\n","  vocab_size_and_embedding_dim = embedding_file_obj.readline().split()\n","\n","  vocab_size = int(vocab_size_and_embedding_dim[0])\n","  dimension_of_embedding = int(vocab_size_and_embedding_dim[1])\n","\n","  for line in embedding_file_obj:\n","    word_n_vector = line.split()\n","    embedding_dict[word_n_vector[0]] = np.array(word_n_vector[1:], dtype=np.float64)\n","\n","  embedding_file_obj.close\n","\n","  #add <UNKNOWN> token to vocabulary\n","  #choose the 0 vector as the corresponding embedding\n","  embedding_dict[\"<UNKNOWN>\"] = np.zeros(dimension_of_embedding, dtype=np.float64)\n","else:\n","  raise Exception(\"Error: embedding not recognized\")"]},{"cell_type":"markdown","metadata":{"id":"EU4ZWl_K_rOK"},"source":["#Feature extraction init"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okNSI1F7BPGy"},"outputs":[],"source":["#feature = average of vectors of tokens occuring in tweet\n","def average_embedding(list_of_sentences, unknown_token_embedding, embedding_model, embedding_model_type):\n","\n","  list_of_features = [] #to return\n","\n","    #for each tweet:\n","    #for each token:\n","    #get embedding vector for the token\n","    #compute average over each tweet\n","    #store it in a list with corresponding label\n","  for tweet in list_of_sentences:\n","    tokenized_tweet = tknzr.tokenize(tweet) #tweet tokenizer, twitter specific, better\n","    #tokenized_tweet = word_tokenize(tweet) #old tokenizer, deprecated\n","    list_of_current_tweet_token_embeddings = []\n","    for token in tokenized_tweet:\n","      embedding_of_current_token = None\n","      \n","      #depending on embedding_model_type, choose appropriate word to vector conversion\n","      if embedding_model_type == \"DICT\":\n","        #check if embedding for a token exists\n","        #if not use the corresponding \"<UNKNOWN>\" token and its embedding \"unknown_token_embedding\"\n","        if token not in embedding_model:\n","          embedding_of_current_token = unknown_token_embedding\n","        else:\n","          embedding_of_current_token = embedding_model.get(token)\n","      elif embedding_model_type == \"NON-DICT\":\n","        #check if embedding for a token exists\n","        #if not use use embedding unknown_token_embedding\n","        if token in embedding_model.vocab:\n","          embedding_of_current_token = embedding_model.get_vector(token)\n","        else:\n","          embedding_of_current_token = unknown_token_embedding\n","      else:\n","        raise Exception(\"Unknown embedding type.\")\n","\n","\n","      list_of_current_tweet_token_embeddings.append(embedding_of_current_token)\n","    #compute average of embeddings of the tokens that make up the tweet\n","    average_embedding_of_current_tweet = np.mean(np.array(list_of_current_tweet_token_embeddings), axis=0)\n","    #append to list of features\n","    list_of_features.append(average_embedding_of_current_tweet)\n","  \n","  return np.array(list_of_features)"]},{"cell_type":"markdown","metadata":{"id":"VXMlp9APzTew"},"source":["#Create training and validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrxzzY3yGlZG"},"outputs":[],"source":["#read file, line by line\n","def read_file_and_strip(filename):\n","  lines = []\n","  with open(filename) as file:\n","    for line in file:\n","      lines.append(line.strip())\n","  return np.array(lines)\n","\n","#read training and validation data, with their respective labels\n","def read_data():\n","  dataset_path = PATH + \"data/\" + PREPROCESSING_CHOICE + \"/\"\n","\n","  train_sentences = read_file_and_strip(dataset_path + \"train_sentences.txt\")\n","  train_labels = read_file_and_strip(dataset_path + \"train_labels.txt\").astype(int)\n","  val_sentences = read_file_and_strip(dataset_path + \"val_sentences.txt\")\n","  val_labels = read_file_and_strip(dataset_path + \"val_labels.txt\").astype(int)\n","  \n","  return train_sentences, train_labels, val_sentences, val_labels\n","\n","train_sentences, train_labels, val_sentences, val_labels = read_data()\n","# print(len(train_sentences))\n","\n","#feature extraction\n","unknown_token_embedding = np.zeros(dimension_of_embedding, dtype=np.float64)\n","embedding_model = None\n","if USE_DICTIONARY:\n","  embedding_model_type = \"DICT\"\n","  embedding_model = embedding_dict\n","else:\n","  embedding_model_type = \"NON-DICT\"\n","  embedding_model = embedding_non_dict_model\n","\n","X_train = average_embedding(train_sentences, unknown_token_embedding, embedding_model, embedding_model_type)\n","X_val = average_embedding(val_sentences, unknown_token_embedding, embedding_model, embedding_model_type)\n","\n","y_train = train_labels\n","y_val = val_labels"]},{"cell_type":"markdown","metadata":{"id":"K8z_mW3Km2K-"},"source":["#Choose and train model"]},{"cell_type":"code","source":["#Find heuristically the longest sequence of tokens over all tokenized tweets\n","#Not relevant for this file but for XLNet and Grubert, for padding purposes.\n","#For \"raw\", max_number_tokens_of_train == 128; max_number_tokens_of_val == 77\n","# max_number_tokens_of_train = max([len(tknzr.tokenize(tweet)) for tweet in train_sentences])\n","# max_number_tokens_of_val = max([len(tknzr.tokenize(tweet)) for tweet in val_sentences])\n","# print(max_number_tokens_of_train)\n","# print(max_number_tokens_of_val)\n"],"metadata":{"id":"-DbE26KP5c5G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Model = XGBoost Classifier on feature vector"],"metadata":{"id":"M5rV3_OlQ3av"}},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","import time\n","\n","#Hyperparameter\n","depth = 12\n","estimators = 128\n","model_name = \"XGB Classifier\" + \"-embedding=\" + get_embedding_name(embedding_choice) + \";-depth=\" + str(depth) + \"estimators=\" + str(estimators)\n","\n","\n","start_time = time.time()\n","#train\n","xgb_model = XGBClassifier(n_estimators=estimators, n_jobs=-1, random_state=42).fit(X_train, y_train) \n","#eval\n","y_val_pred = xgb_model.predict(X_val)\n","end_time = time.time()\n","elapsed_time_in_seconds = end_time - start_time\n","print(\"Training and evaluation duration: %d s\" %elapsed_time_in_seconds)\n","\n","# save_stats_to_file(y_val, y_val_pred, model_name)\n","# save_model(xgb_model, model_name)"],"metadata":{"id":"tkQSPs9zQ2Th","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659012362406,"user_tz":-120,"elapsed":2698231,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"}},"outputId":"7b6c9b87-e113-44e9-d37f-a99285690824"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training and evaluation duration: 2698 s\n"]}]},{"cell_type":"markdown","source":["###Eval and stats"],"metadata":{"id":"SHz0A-WxRCBo"}},{"cell_type":"code","source":["print(f'Acc: {accuracy_score(y_val, y_val_pred)}')\n","print(f'Recall: {recall_score(y_val, y_val_pred)}')\n","print(f'Precision: {precision_score(y_val, y_val_pred)}')\n","print(f'F1: {f1_score(y_val, y_val_pred)}')\n","print(f'ROC_AUC: {roc_auc_score(y_val, y_val_pred)}')"],"metadata":{"id":"NKb9s7M6Q-kq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659012362752,"user_tz":-120,"elapsed":363,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"}},"outputId":"26264591-ab2d-4711-f1ac-4a15da3c94cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Acc: 0.7371933547370694\n","Recall: 0.765408885790806\n","Precision: 0.7250808049212804\n","F1: 0.7446992682491522\n","ROC_AUC: 0.7371496131486897\n"]}]},{"cell_type":"code","source":["save_stats_to_file(y_val, y_val_pred, model_name)"],"metadata":{"id":"2sUneyPVQ-Y8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Save Model"],"metadata":{"id":"P-1gffLcRGbM"}},{"cell_type":"code","source":["save_model(xgb_model, model_name)"],"metadata":{"id":"jRMV5JkCRH2T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5isaz7VhZyM8"},"source":["##Model = Log Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46974,"status":"ok","timestamp":1658676096221,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"},"user_tz":-120},"id":"ilpc7IeOKLFM","outputId":"775736fa-2027-403f-f35c-10506b50270e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training and evaluation duration: 46 s\n"]}],"source":["from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n","from sklearn.svm import LinearSVC\n","import time\n","start_time = time.time()\n","\n","#hyperparameter\n","C = 1\n","max_iter = 1000\n","\n","# model = RidgeClassifier(alpha=1.0)\n","model_LR = LogisticRegression(C=C, max_iter=max_iter)\n","# model = SGDClassifier()\n","# model = LinearSVC(C=1.0)\n","\n","model_name = \"1-LogisticRegression-\" + \"C=\" + str(C) + \"-max_iter=\" + str(max_iter) \n","#train\n","model_LR.fit(X_train, y_train)\n","#eval\n","y_val_pred = model_LR.predict(X_val)\n","end_time = time.time()\n","elapsed_time_in_seconds = end_time - start_time\n","print(\"Training and evaluation duration: %d s\" %elapsed_time_in_seconds)"]},{"cell_type":"markdown","metadata":{"id":"sokGvafnm98X"},"source":["###Eval and stats"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":539,"status":"ok","timestamp":1658676096751,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"},"user_tz":-120},"id":"j6qvIhcoyvF3","outputId":"b697ee5f-bd3f-49a6-f6f0-0329c64559e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving model relevant stats to file.\n","Acc: 0.7427638379851518\n","Recall: 0.7523502399743525\n","Precision: 0.7369446753486654\n","F1: 0.7445677785398673\n","ROC_AUC: 0.7427958923231976\n"]}],"source":["#adapt name when changing model\n","save_stats_to_file(y_val, y_val_pred, model_name)\n","#saves stats and model to current directory"]},{"cell_type":"markdown","source":["###Save model"],"metadata":{"id":"89_Qn9PkruJ6"}},{"cell_type":"code","source":["save_model(model_LR, model_name)"],"metadata":{"id":"Ri0WLVNhr0kq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OK8vTpDJZ62M"},"source":["##Model = Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4081067,"status":"ok","timestamp":1658438226637,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"},"user_tz":-120},"id":"54iHEVpzD4wc","outputId":"13262278-a47d-404f-e9cf-b4a246960c90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training and evaluation duration: 4081 s\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","import time\n","start_time = time.time()\n","\n","#hyerparameters\n","n_estimators = 128\n","random_state = 42\n","\n","model_name = \"RandomForestClassifier-\" + \"n_estimators=\" + str(n_estimators) + \"-random_state=\" + str(random_state)\n","\n","#model init\n","model_RFC = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state, n_jobs=-1)\n","\n","#train\n","model_RFC.fit(X_train, y_train)\n","#eval\n","y_val_pred = model_RFC.predict(X_val)\n","end_time = time.time()\n","elapsed_time_in_seconds = end_time - start_time\n","print(\"Training and evaluation duration: %d s\" %elapsed_time_in_seconds)"]},{"cell_type":"markdown","metadata":{"id":"AxdxsNjsaKJV"},"source":["###Eval and stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULU8GOGIy90f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658438227701,"user_tz":-120,"elapsed":1153,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"}},"outputId":"719196e4-4945-45ad-fec3-6ce78239e627"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving model relevant stats to file.\n","Acc: 0.7480683455705758\n","Recall: 0.7936673244171599\n","Precision: 0.7278469877600261\n","F1: 0.7593334702635615\n","ROC_AUC: 0.74799877569604\n"]}],"source":["#adapt name when changing model\n","save_stats_to_file(y_val, y_val_pred, model_name)\n","#saves stats and model to current directory"]},{"cell_type":"markdown","source":["###Save model"],"metadata":{"id":"r1If5idAsA3Y"}},{"cell_type":"code","source":["save_model(model_RFC, model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpMjyUY1sCq3","executionInfo":{"status":"ok","timestamp":1657372865577,"user_tz":-120,"elapsed":18466,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"}},"outputId":"552e3bed-39f1-4f61-b83e-911a23f89423"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved.\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Embedding + (Log Regression + Random Forest + XGBoost).ipynb","provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}