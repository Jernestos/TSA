{"cells":[{"cell_type":"markdown","metadata":{"id":"KA7QY9gVRZbQ"},"source":["#Init and load data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31066,"status":"ok","timestamp":1658046658215,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"},"user_tz":-120},"id":"H_lYrMfeQ31Y","outputId":"fcc20346-54ea-4919-856e-efeab777154d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1NSBJKoA0FA5BeDWR8bNo5tPgA5lCU8yv/CIL 2022\n","'Bert inspiration'\n"," BoW_v2.ipynb\n"," code\n"," data\n"," Models\n"," Notes.gdoc\n"," On_Processed_Bag-of-words_baseline.ipynb\n"," papers_to_possible_reference\n"," Preprocessing\n"," Project_2_Bag-of-words_baseline.ipynb\n"," __pycache__\n"," resources\n"," Results.gsheet\n"," template.ipynb\n","'trained Glove_LogisticRegression(C=1, max_iter=1000).pkl'\n"," trash\n","'Word embeddings'\n"," XGBoost\n","Choosing data: with-stemming_with-lemmatize_with-stopwords_no-spellcorrect\n","Init. methods.\n","Loading data.\n"]}],"source":["#In this cell, there are various option for the user to freely choose:\n","#embedding_choice\n","#PREPROCESSING_CHOICE\n","\n","import pandas as pd\n","import numpy as np\n","\n","use_drive = True\n","#for euler, remove this entire if else branch and set PATH to \"./\"\n","if use_drive:\n","  PATH = \"/content/drive/MyDrive/CIL 2022/\"\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  %cd /content/drive/MyDrive/CIL 2022/\n","  !ls\n","else:\n","  PATH = \"./\"\n","\n","\n","print(\"Choosing data: \", end=\"\")\n","#option 0 - 8\n","PREPROCESSING_OPTIONS = [ \"raw\",\n","\"no-stemming_no-lemmatize_no-stopwords_no-spellcorrect\",\n","\"no-stemming_no-lemmatize_with-stopwords_no-spellcorrect\",\n","\"no-stemming_no-lemmatize_with-stopwords_with-spellcorrect\",\n","\"no-stemming_with-lemmatize_with-stopwords_no-spellcorrect\",\n","\"no-stemming_with-lemmatize_with-stopwords_with-spellcorrect\",\n","\"with-stemming_no-lemmatize_with-stopwords_no-spellcorrect\",\n","\"with-stemming_with-lemmatize_no-stopwords_with-spellcorrect\",\n","\"with-stemming_with-lemmatize_with-stopwords_no-spellcorrect\" ]\n","PREPROCESSING_CHOICE = PREPROCESSING_OPTIONS[8] # one from PREPROCESSING_OPTIONS\n","print(PREPROCESSING_CHOICE)\n","\n","\n","\n","print(\"Init. methods.\")\n","#helper function to save models and stats\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n","import pickle\n","\n","#save model\n","# now you can save it to a file\n","# with open('filename.pkl', 'wb') as f:\n","#     pickle.dump(clf, f)\n","\n","# # and later you can load it\n","# with open('filename.pkl', 'rb') as f:\n","#     clf = pickle.load(f)\n","\n","#Saving trained model\n","def save_model(model, model_name):\n","  name_of_file_model = \"trained \" + model_name + \".pkl\"\n","  with open(name_of_file_model, 'wb') as f:\n","    pickle.dump(model, f)\n","  print(\"Model saved.\")\n","\n","#Saving stats of model to a file\n","def save_stats_to_file(y_val, y_val_pred, model_name):\n","  name_of_file_stats = model_name + PREPROCESSING_CHOICE + \".txt\"\n","  # name_of_file_model = \"trained \" + model_name + \".pkl\"\n","  # print(\"Saving model to file.\")\n","  # with open(name_of_file_model, 'wb') as f:\n","  #   pickle.dump(model, f)\n","  print(\"Saving model relevant stats to file.\")\n","  file_obj = open(name_of_file_stats, \"w\", encoding=\"utf8\")\n","  file_obj.write(f'use_drive: {use_drive}\\n')\n","  # file_obj.write(f'LOAD_FROM_SPLITTED_DATASET: {LOAD_FROM_SPLITTED_DATASET}\\n')\n","  # file_obj.write(f'WRITE_SPLITTED_DATASET: {WRITE_SPLITTED_DATASET}\\n')\n","  # file_obj.write(f'REMOVE_DUPLICATES_FROM_TRAINING: {REMOVE_DUPLICATES_FROM_TRAINING}\\n')\n","  file_obj.write(f'PREPROCESSING_CHOICE: {PREPROCESSING_CHOICE}\\n')\n","  file_obj.write(f'Model name: {model_name}\\n')\n","  file_obj.write(f'Acc: {accuracy_score(y_val, y_val_pred)}\\n')\n","  file_obj.write(f'Recall: {recall_score(y_val, y_val_pred)}\\n')\n","  file_obj.write(f'Precision: {precision_score(y_val, y_val_pred)}\\n')\n","  file_obj.write(f'F1: {f1_score(y_val, y_val_pred)}\\n')\n","  file_obj.write(f'ROC_AUC: {roc_auc_score(y_val, y_val_pred)}\\n')\n","  file_obj.close()\n","  print(f'Acc: {accuracy_score(y_val, y_val_pred)}')\n","  print(f'Recall: {recall_score(y_val, y_val_pred)}')\n","  print(f'Precision: {precision_score(y_val, y_val_pred)}')\n","  print(f'F1: {f1_score(y_val, y_val_pred)}')\n","  print(f'ROC_AUC: {roc_auc_score(y_val, y_val_pred)}')\n","\n","def read_file_and_strip(filename):\n","  lines = []\n","  with open(filename) as file:\n","    for line in file:\n","      lines.append(line.strip())\n","  return np.array(lines)\n","\n","def read_data():\n","  dataset_path = PATH + \"data/\" + PREPROCESSING_CHOICE + \"/\"\n","\n","  train_sentences = read_file_and_strip(dataset_path + \"train_sentences.txt\")\n","  train_labels = read_file_and_strip(dataset_path + \"train_labels.txt\").astype(int)\n","  val_sentences = read_file_and_strip(dataset_path + \"val_sentences.txt\")\n","  val_labels = read_file_and_strip(dataset_path + \"val_labels.txt\").astype(int)\n","  \n","  return train_sentences, train_labels, val_sentences, val_labels\n","\n","print(\"Loading data.\")\n","\n","train_sentences, train_labels, val_sentences, val_labels = read_data()"]},{"cell_type":"markdown","metadata":{"id":"xxd1hGwxR3tr"},"source":["#XGBoost Classifier"]},{"cell_type":"markdown","metadata":{"id":"1H42Cj7CxDUP"},"source":["##XGBoost Classifier + CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1kue46CWUSL"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","features = 100000\n","vectorizer = CountVectorizer(max_features = features)\n","#vectorizer = TfidfVectorizer()\n","\n","\n","X_train = vectorizer.fit_transform(train_sentences)\n","X_val = vectorizer.transform(val_sentences)\n","\n","y_train = train_labels\n","y_val = val_labels\n","\n","# print(len(X_train))\n","# print(len(y_train))\n","# print(vectorizer.shape)"]},{"cell_type":"code","source":["# XGB Classifier on training data\n","\n","from xgboost import XGBClassifier\n","import time\n","\n","depth = 12\n","estimators = 1024 + 512\n","model_name = \"Features=\" + str(features) + \" \" + \"XGB Classifier\" + \" estimators=\" + str(estimators) + \" depth=\" + str(depth)\n","#model_name = str(features) + \" \" + \"XGB Classifier\" + \" \" + str(depth) + \" \" + str(estimators)\n","#model_name = \"Features=\" + str(features) + \" \" + \"XGB Classifier\" + \" estimators=\" + str(estimators)\n","#model_name = \"TfidfVectorizer -\" + \"XGB Classifier\" + \" estimators=\" + str(estimators)\n","#model_name = \"TfidfVectorizer -\" + \"XGB Classifier\" + \" estimators=\" + str(estimators) + \" depth=\" + str(depth)\n","\n","start_time = time.time()\n","#xgb_model = XGBClassifier(max_depth=depth, n_estimators=estimators, n_jobs=2, random_state=42).fit(X_train, y_train) \n","xgb_model = XGBClassifier(n_estimators=estimators, max_depth=depth, n_jobs=2, random_state=42).fit(X_train, y_train) \n","predicted_y_label = xgb_model.predict(X_val)\n","end_time = time.time()\n","elapsed_time_in_seconds = end_time - start_time\n","print(\"Training and evaluation duration: %d s\" %elapsed_time_in_seconds)\n","\n","model_name = str(elapsed_time_in_seconds) + \"s - \" + model_name\n","save_stats_to_file(y_val, predicted_y_label, model_name)\n","save_model(xgb_model, model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jdS9zysU4aoK","executionInfo":{"status":"ok","timestamp":1658083249007,"user_tz":-120,"elapsed":9671101,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"}},"outputId":"37d46752-ccb7-4191-cbc4-4082bede65b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training and evaluation duration: 9669 s\n","Saving model relevant stats to file.\n","Acc: 0.8078775818294106\n","Recall: 0.8500054812541109\n","Precision: 0.7843757112870186\n","F1: 0.8158728906075313\n","ROC_AUC: 0.8078133077436401\n","Model saved.\n"]}]},{"cell_type":"markdown","source":["##XGBoost Classifier + TfidfVectorizer"],"metadata":{"id":"xCZwHwDxOeAm"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","# features = 100000\n","# vectorizer = CountVectorizer(max_features = features)\n","vectorizer = TfidfVectorizer()\n","\n","\n","X_train = vectorizer.fit_transform(train_sentences)\n","X_val = vectorizer.transform(val_sentences)\n","\n","y_train = train_labels\n","y_val = val_labels\n","\n","# print(len(X_train))\n","# print(len(y_train))\n","# print(vectorizer.shape)"],"metadata":{"id":"QZJXeM0PIET4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# XGB Classifier on training data\n","\n","from xgboost import XGBClassifier\n","import time\n","\n","depth = 12\n","estimators = 1024 + 512\n","model_name = \"Features=\" + str(features) + \" \" + \"XGB Classifier\" + \" estimators=\" + str(estimators) + \" depth=\" + str(depth)\n","#model_name = str(features) + \" \" + \"XGB Classifier\" + \" \" + str(depth) + \" \" + str(estimators)\n","#model_name = \"Features=\" + str(features) + \" \" + \"XGB Classifier\" + \" estimators=\" + str(estimators)\n","#model_name = \"TfidfVectorizer -\" + \"XGB Classifier\" + \" estimators=\" + str(estimators)\n","#model_name = \"TfidfVectorizer -\" + \"XGB Classifier\" + \" estimators=\" + str(estimators) + \" depth=\" + str(depth)\n","\n","start_time = time.time()\n","#xgb_model = XGBClassifier(max_depth=depth, n_estimators=estimators, n_jobs=2, random_state=42).fit(X_train, y_train) \n","xgb_model = XGBClassifier(n_estimators=estimators, max_depth=depth, n_jobs=2, random_state=42).fit(X_train, y_train) \n","predicted_y_label = xgb_model.predict(X_val)\n","end_time = time.time()\n","elapsed_time_in_seconds = end_time - start_time\n","print(\"Training and evaluation duration: %d s\" %elapsed_time_in_seconds)\n","\n","model_name = str(elapsed_time_in_seconds) + \"s - \" + model_name\n","save_stats_to_file(y_val, predicted_y_label, model_name)\n","save_model(xgb_model, model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jeDJZw5uOTX3","executionInfo":{"status":"ok","timestamp":1658002084564,"user_tz":-120,"elapsed":6575141,"user":{"displayName":"Jupin Visis","userId":"10701197737852651398"}},"outputId":"652ebb98-44d1-437a-9b6d-ed6a123b2103"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training and evaluation duration: 6574 s\n","Saving model relevant stats to file.\n","Acc: 0.8047073354834282\n","Recall: 0.8486260323028576\n","Precision: 0.7805300216781219\n","F1: 0.8131548770559966\n","ROC_AUC: 0.804640329197218\n","Model saved.\n"]}]}],"metadata":{"colab":{"name":"XGBoost.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyODppg5n0X4XPWVdR8KmySm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}